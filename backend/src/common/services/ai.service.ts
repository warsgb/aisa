import { Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import OpenAI from 'openai';

export interface Message {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

export interface StreamOptions {
  messages: Message[];
  system?: string;
  maxTokens?: number;
  temperature?: number;
  model?: string;
  onChunk?: (chunk: string) => void;
  onComplete?: (fullText: string) => void;
  onError?: (error: Error) => void;
  onStart?: () => void;
}

@Injectable()
export class AIService {
  private readonly logger = new Logger(AIService.name);
  private client: OpenAI;
  private readonly provider: string;
  private readonly defaultModel: string;
  private readonly defaultMaxTokens: number;
  private readonly defaultTemperature: number;

  constructor(private configService: ConfigService) {
    this.provider = this.configService.get<string>('AI_PROVIDER', 'zhipu');

    if (this.provider === 'zhipu') {
      const apiKey = this.configService.get<string>('ZHIPU_API_KEY');
      const baseURL = this.configService.get<string>('ZHIPU_BASE_URL', 'https://open.bigmodel.cn/api/paas/v4/');
      this.defaultModel = this.configService.get<string>('ZHIPU_MODEL', 'glm-4-plus');
      this.defaultMaxTokens = this.configService.get<number>('ZHIPU_MAX_TOKENS', 4096);
      this.defaultTemperature = this.configService.get<number>('ZHIPU_TEMPERATURE', 0.7);

      if (!apiKey || apiKey === 'your_zhipu_api_key_here') {
        this.logger.warn('ZHIPU_API_KEY not configured or using placeholder. Will use mock response mode.');
        this.client = null as any; // Mark as not configured
      } else {
        // æ™ºè°±AIä½¿ç”¨OpenAIå…¼å®¹æ¥å£
        this.client = new OpenAI({
          apiKey,
          baseURL,
        });
      }

      this.logger.log(`AI Service initialized (Zhipu GLM) with model: ${this.defaultModel}`);
      this.logger.log(`Base URL: ${baseURL}`);
    } else {
      // Fallback to Anthropic (if needed in future)
      const apiKey = this.configService.get<string>('ANTHROPIC_API_KEY');
      const baseURL = this.configService.get<string>('ANTHROPIC_BASE_URL');
      this.defaultModel = this.configService.get<string>('ANTHROPIC_MODEL', 'claude-3-5-sonnet-20241022');

      if (!apiKey) {
        this.logger.warn('ANTHROPIC_API_KEY not configured');
      }

      const clientConfig: any = { apiKey };
      if (baseURL) {
        clientConfig.baseURL = baseURL;
      }

      // Note: This would need Anthropic SDK, for now using OpenAI-compatible format
      this.client = new OpenAI(clientConfig);
      this.logger.log(`AI Service initialized (Anthropic-compatible) with model: ${this.defaultModel}`);
    }
  }

  async stream(options: StreamOptions): Promise<string> {
    console.log('ğŸ¤– [AI Service] Stream called with options:', {
      model: options.model,
      maxTokens: options.maxTokens,
      temperature: options.temperature,
      messagesCount: options.messages?.length || 0,
    });

    const {
      messages,
      system,
      maxTokens = this.defaultMaxTokens,
      temperature = this.defaultTemperature,
      model = this.defaultModel,
      onChunk,
      onComplete,
      onError,
      onStart,
    } = options;

    try {
      // Check if AI client is configured
      if (!this.client) {
        this.logger.warn('AI client not configured, using mock response');

        // Generate a mock response
        const mockResponse = this.generateMockResponse(messages, system);

        // Simulate streaming by sending chunks
        if (onStart) {
          onStart();
        }

        if (onChunk) {
          const chunkSize = 20;
          for (let i = 0; i < mockResponse.length; i += chunkSize) {
            const chunk = mockResponse.substring(i, i + chunkSize);
            onChunk(chunk);
            // Add small delay to simulate streaming
            await new Promise(resolve => setTimeout(resolve, 50));
          }
        }

        if (onComplete) {
          onComplete(mockResponse);
        }

        return mockResponse;
      }

      // Prepare messages
      const allMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [];

      if (this.provider === 'zhipu') {
        // æ™ºè°±AI: å°†system promptåˆå¹¶åˆ°ç¬¬ä¸€æ¡useræ¶ˆæ¯ä¸­
        // å› ä¸ºæ™ºè°±AIçš„APIä¸æ”¯æŒæˆ–ä¸éœ€è¦systemè§’è‰²
        const filteredMessages = messages
          .filter((m) => m.role !== 'system')
          .map((m) => ({
            role: m.role as 'user' | 'assistant',
            content: m.content,
          }));

        // å¦‚æœæœ‰system promptï¼Œæ·»åŠ åˆ°ç¬¬ä¸€æ¡useræ¶ˆæ¯å‰
        if (system && filteredMessages.length > 0) {
          const firstMessage = filteredMessages[0];
          if (firstMessage.role === 'user') {
            // åˆå¹¶systemå’Œç¬¬ä¸€æ¡useræ¶ˆæ¯
            filteredMessages[0] = {
              role: 'user',
              content: `${system}\n\n${firstMessage.content}`,
            };
          }
        } else if (system && filteredMessages.length === 0) {
          // å¦‚æœåªæœ‰systemæ²¡æœ‰å…¶ä»–æ¶ˆæ¯ï¼Œå°†systemä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯
          filteredMessages.push({
            role: 'user',
            content: system,
          });
        }

        allMessages.push(...filteredMessages);
      } else {
        // å…¶ä»–æä¾›å•†(å¦‚Anthropic): ä½¿ç”¨æ ‡å‡†systemè§’è‰²
        if (system) {
          allMessages.push({
            role: 'system',
            content: system,
          });
        }

        const filteredMessages = messages
          .filter((m) => m.role !== 'system')
          .map((m) => ({
            role: m.role as 'user' | 'assistant',
            content: m.content,
          }));

        allMessages.push(...filteredMessages);
      }

      this.logger.log(`ğŸ“¤ [AI Service] Sending ${allMessages.length} messages to ${this.provider} API`);
      this.logger.debug(`ğŸ“¨ Message preview:`, JSON.stringify(allMessages).substring(0, 200));
      this.logger.debug(`Messages: ${JSON.stringify(allMessages).substring(0, 200)}...`);

      if (onStart) {
        onStart();
      }

      // Call streaming API
      console.log('ğŸŒ [AI Service] Calling Zhipu API:', {
        endpoint: this.client.baseURL,
        model,
        maxTokens,
      });

      const stream = await this.client.chat.completions.create({
        model,
        messages: allMessages,
        max_tokens: maxTokens,
        temperature,
        stream: true,
      });

      let fullText = '';
      let chunkCount = 0;

      console.log('ğŸ¬ [AI Service] Starting to receive stream...');

      for await (const chunk of stream) {
        const delta = chunk.choices[0]?.delta?.content || '';
        fullText += delta;
        chunkCount++;

        if (chunkCount % 10 === 0) {
          console.log(`ğŸ“Š [AI Service] Received ${chunkCount} chunks`);
        }

        if (onChunk) {
          onChunk(delta);
        }
      }

      if (onComplete) {
        console.log('âœ… [AI Service] Stream completed:', {
          totalChunks: chunkCount,
          totalLength: fullText.length,
        });
        onComplete(fullText);
      }

      console.log('ğŸ [AI Service] Returning response to skill executor');

      return fullText;
    } catch (error) {
      this.logger.error('AI API error:', error);
      if (onError) {
        onError(error as Error);
      }
      throw error;
    }
  }

  async create(options: StreamOptions): Promise<string> {
    const {
      messages,
      system,
      maxTokens = this.defaultMaxTokens,
      temperature = this.defaultTemperature,
      model = this.defaultModel,
    } = options;

    try {
      const allMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [];

      if (this.provider === 'zhipu') {
        // æ™ºè°±AI: å°†system promptåˆå¹¶åˆ°ç¬¬ä¸€æ¡useræ¶ˆæ¯ä¸­
        const filteredMessages = messages
          .filter((m) => m.role !== 'system')
          .map((m) => ({
            role: m.role as 'user' | 'assistant',
            content: m.content,
          }));

        // å¦‚æœæœ‰system promptï¼Œæ·»åŠ åˆ°ç¬¬ä¸€æ¡useræ¶ˆæ¯å‰
        if (system && filteredMessages.length > 0) {
          const firstMessage = filteredMessages[0];
          if (firstMessage.role === 'user') {
            filteredMessages[0] = {
              role: 'user',
              content: `${system}\n\n${firstMessage.content}`,
            };
          }
        } else if (system && filteredMessages.length === 0) {
          // å¦‚æœåªæœ‰systemæ²¡æœ‰å…¶ä»–æ¶ˆæ¯ï¼Œå°†systemä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯
          filteredMessages.push({
            role: 'user',
            content: system,
          });
        }

        allMessages.push(...filteredMessages);
      } else {
        // å…¶ä»–æä¾›å•†: ä½¿ç”¨æ ‡å‡†systemè§’è‰²
        if (system) {
          allMessages.push({
            role: 'system',
            content: system,
          });
        }

        allMessages.push(
          ...messages
            .filter((m) => m.role !== 'system')
            .map((m) => ({
              role: m.role as 'user' | 'assistant',
              content: m.content,
            }))
        );
      }

      const response = await this.client.chat.completions.create({
        model,
        messages: allMessages,
        max_tokens: maxTokens,
        temperature,
      });

      return response.choices[0]?.message?.content || '';
    } catch (error) {
      this.logger.error('AI API error:', error);
      throw error;
    }
  }

  /**
   * æ‰§è¡ŒWebæœç´¢ï¼ˆæ™ºè°±AI WebSearch APIï¼‰
   * @param query æœç´¢æŸ¥è¯¢
   * @param options æœç´¢é€‰é¡¹
   * @returns æœç´¢ç»“æœ
   */
  async webSearch(
    query: string,
    options?: {
      searchEngine?: 'search_std' | 'search_pro' | 'search_pro_sogou' | 'search_pro_quark';
      count?: number;
      searchRecencyFilter?: 'noLimit' | 'day' | 'week' | 'month' | 'year';
      contentSize?: 'low' | 'medium' | 'high';
    },
  ): Promise<{ title: string; link: string; content: string }[]> {
    const {
      count = 10,
    } = options || {};

    this.logger.log(`ğŸ” [WebSearch] Starting search: "${query}"`);
    this.logger.log(`   [WebSearch] Count: ${count}`);

    try {
      // Check if AI client is configured
      if (!this.client) {
        this.logger.warn('[WebSearch] AI client not configured, returning empty search results');
        return [];
      }

      // ä½¿ç”¨æ™ºè°±AIçš„WebSearch API
      // æ ¹æ®å®˜æ–¹æ–‡æ¡£ä½¿ç”¨ web_search ç±»å‹
      this.logger.log(`ğŸŒ [WebSearch] Calling Zhipu WebSearch API...`);

      const {
        searchEngine = 'search_std',
        searchRecencyFilter = 'noLimit',
        contentSize = 'medium',
      } = options || {};

      this.logger.log(`ğŸ“ [WebSearch] Query: "${query}"`);
      this.logger.log(`ğŸ“ [WebSearch] Engine: ${searchEngine}, Count: ${count}, Recency: ${searchRecencyFilter}`);

      const response = await this.client.chat.completions.create({
        model: this.defaultModel,
        messages: [
          {
            role: 'user',
            content: `è¯·å¸®æˆ‘æœç´¢å…³äº"${query}"çš„ä¿¡æ¯ã€‚`
          }
        ],
        tools: [
          {
            type: 'web_search',
            web_search: {
              search_query: query,
              search_engine: searchEngine,
              enable: true,  // å¿…é¡»è®¾ä¸ºtrueæ‰èƒ½å¯ç”¨æœç´¢
              count: count,
              search_recency_filter: searchRecencyFilter,
              content_size: contentSize,
              search_result: true,  // è¿”å›æœç´¢æ¥æºçš„è¯¦ç»†ä¿¡æ¯
            },
          } as any,
        ],
        tool_choice: 'auto',
      } as any);

      // è§£ææœç´¢ç»“æœ
      const message = response.choices[0]?.message;
      const toolCalls = message?.tool_calls;

      this.logger.log(`ğŸ“¥ [WebSearch] API Response keys:`, Object.keys(response.choices[0] || {}));
      this.logger.log(`ğŸ“¥ [WebSearch] Message keys:`, Object.keys(message || {}));
      this.logger.log(`ğŸ“¥ [WebSearch] Tool calls: ${toolCalls?.length || 0}`);
      this.logger.log(`ğŸ“¥ [WebSearch] Full response:`, JSON.stringify(response.choices[0], null, 2));

      // web_search å·¥å…·çš„ç»“æœå¯èƒ½ç›´æ¥åœ¨ message.content ä¸­
      // æˆ–è€…éœ€è¦é€šè¿‡ tool_calls è·å–
      const results: { title: string; link: string; content: string }[] = [];

      // é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ç›´æ¥çš„ content å“åº”
      const content = message?.content;
      if (content && content.trim()) {
        this.logger.log(`ğŸ“¥ [WebSearch] Got content response: ${content.substring(0, 200)}...`);
        // AIå·²ç»åŸºäºæœç´¢ç»“æœç”Ÿæˆäº†å›ç­”ï¼Œç›´æ¥è¿”å›
        results.push({
          title: 'AIæœç´¢ç»“æœ',
          link: '',
          content: content
        });
      }

      // å¦‚æœæœ‰ tool_callsï¼Œå°è¯•ä»ä¸­æå–æœç´¢ç»“æœè¯¦æƒ…
      if (toolCalls && toolCalls.length > 0) {
        this.logger.log(`ğŸ“¥ [WebSearch] Processing ${toolCalls.length} tool calls`);

        for (const toolCall of toolCalls) {
          const func = (toolCall as any).function;
          if (func) {
            this.logger.log(`ğŸ“Š [WebSearch] Tool call function name: ${func.name}`);
            this.logger.log(`ğŸ“Š [WebSearch] Function arguments: ${func.arguments}`);

            // web_search å¯èƒ½è¿”å›å„ç§ function name
            // å°è¯•è§£æå‚æ•°è·å–ç»“æ„åŒ–æœç´¢ç»“æœ
            try {
              const args = JSON.parse(func.arguments);
              this.logger.log(`ğŸ“Š [WebSearch] Parsed arguments:`, JSON.stringify(args, null, 2));

              // æ£€æŸ¥æ˜¯å¦æœ‰æœç´¢ç»“æœåˆ—è¡¨
              const searchResults = args.search_results || args.results || [];

              if (Array.isArray(searchResults) && searchResults.length > 0) {
                this.logger.log(`ğŸ“Š [WebSearch] Found ${searchResults.length} structured results`);
                for (const item of searchResults) {
                  results.push({
                    title: item.title || '',
                    link: item.url || item.link || item.media_name || '',
                    content: item.content || item.snippet || item.description || ''
                  });
                }
              }
            } catch (e) {
              this.logger.error('[WebSearch] Failed to parse tool arguments:', e);
            }
          }
        }
      }

      if (results.length === 0) {
        this.logger.warn('[WebSearch] No results found in response');
        return [];
      }

      this.logger.log(`âœ… [WebSearch] Completed: "${query}" -> ${results.length} results`);
      return results;
    } catch (error) {
      this.logger.error(`âŒ [WebSearch] API error for "${query}":`, error);
      return []; // è¿”å›ç©ºæ•°ç»„è€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯ï¼Œç¡®ä¿æŠ€èƒ½å¯ä»¥ç»§ç»­æ‰§è¡Œ
    }
  }

  /**
   * æ‰§è¡Œå¤šä¸ªæœç´¢æŸ¥è¯¢å¹¶åˆå¹¶ç»“æœï¼ˆå¹¶è¡Œæ‰§è¡Œä¼˜åŒ–ç‰ˆï¼‰
   * @param queries æœç´¢æŸ¥è¯¢åˆ—è¡¨
   * @param options æœç´¢é€‰é¡¹
   * @returns åˆå¹¶åçš„æœç´¢ç»“æœ
   */
  async webSearchMultiple(
    queries: string[],
    options?: {
      maxConcurrency?: number;
      searchEngine?: 'search_std' | 'search_pro' | 'search_pro_sogou' | 'search_pro_quark';
      count?: number;
      searchRecencyFilter?: 'noLimit' | 'day' | 'week' | 'month' | 'year';
      contentSize?: 'low' | 'medium' | 'high';
    },
  ): Promise<{ query: string; results: { title: string; link: string; content: string }[] }[]> {
    const { maxConcurrency = 5 } = options || {};
    this.logger.log(`ğŸ” [WebSearch] Executing ${queries.length} search queries with maxConcurrency=${maxConcurrency}`);

    const allResults: { query: string; results: { title: string; link: string; content: string }[] }[] = [];

    // åˆ†æ‰¹å¹¶è¡Œæ‰§è¡Œï¼Œé¿å…è§¦å‘APIé™æµ
    for (let i = 0; i < queries.length; i += maxConcurrency) {
      const batch = queries.slice(i, i + maxConcurrency);
      this.logger.log(`ğŸ“¦ [WebSearch] Processing batch ${Math.floor(i / maxConcurrency) + 1} with ${batch.length} queries`);

      // å¹¶è¡Œæ‰§è¡Œå½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰æœç´¢
      const batchPromises = batch.map(q => this.webSearch(q, options));
      const batchResults: { title: string; link: string; content: string }[][] = await Promise.all(batchPromises);

      // æ”¶é›†æ‰¹æ¬¡ç»“æœ
      allResults.push(...batch.map((query, idx) => ({
        query,
        results: batchResults[idx]
      })));

      // æ‰¹æ¬¡é—´æ·»åŠ å°å»¶è¿Ÿï¼Œè¿›ä¸€æ­¥é¿å…é™æµ
      if (i + maxConcurrency < queries.length) {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }

    this.logger.log(`âœ… [WebSearch] Completed all ${queries.length} searches in parallel`);
    return allResults;
  }

  private generateMockResponse(messages: Message[], system?: string): string {
    // Extract parameters from the last user message if present
    const lastUserMessage = messages.filter((m) => m.role === 'user').pop();
    let response = '';

    if (lastUserMessage && lastUserMessage.content.includes('ç›®æ ‡è§’è‰²')) {
      // Elevator pitch skill mock response
      response = `# ç”µæ¢¯æ¼”è®²ï¼š30ç§’æ‰“åŠ¨CEO

**CEOæ‚¨å¥½ï¼Œç»™æˆ‘30ç§’æ—¶é—´ï¼š**

ä½œä¸ºå»ºç­‘è¡Œä¸šçš„é¢†å†›ä¼ä¸šï¼ŒåŒ—äº¬å»ºå·¥é›†å›¢æ­£åœ¨æ¨è¿›æ•°å­—åŒ–è½¬å‹ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæ‚¨çš„å›¢é˜Ÿèƒ½å¤Ÿï¼š

âœ¨ **æå‡3å€å·¥ä½œæ•ˆç‡** - æ™ºèƒ½æ–‡æ¡£åä½œï¼Œè®©é¡¹ç›®èµ„æ–™å®æ—¶åŒæ­¥
ğŸš€ **ç¼©çŸ­50%å®¡æ‰¹å‘¨æœŸ** - æµç¨‹è‡ªåŠ¨åŒ–ï¼Œä»ç«‹é¡¹åˆ°éªŒæ”¶å…¨é¢æé€Ÿ
ğŸ’¡ **é™ä½70%æ²Ÿé€šæˆæœ¬** - è·¨éƒ¨é—¨åä½œæ— ç¼è¡”æ¥ï¼Œä¿¡æ¯é›¶å»¶è¿Ÿ

WPS 365å·²æœåŠ¡è¶…è¿‡500å®¶å»ºç­‘å›½ä¼ï¼ŒåŒ…æ‹¬ä¸­å»ºã€ä¸­é“ç­‰é¾™å¤´ä¼ä¸šã€‚æˆ‘ä»¬çš„å¹³å°æ­£åœ¨å¸®åŠ©æ‚¨çš„åŒè¡Œå®ç°**"é™æœ¬å¢æ•ˆã€å®‰å…¨å¯æ§"**çš„æ•°å­—åŒ–ç›®æ ‡ã€‚

**ä¸‹å‘¨ä¸€ä¸Šåˆ10ç‚¹ï¼Œæˆ‘èƒ½ç”¨15åˆ†é’Ÿä¸ºæ‚¨å±•ç¤ºå…·ä½“æ¡ˆä¾‹å—ï¼Ÿ**

---
*è¿™å°±æ˜¯æ„¿æ™¯å‹é’©å­çš„åŠ›é‡ - ä¸æ˜¯æ¨é”€äº§å“ï¼Œè€Œæ˜¯æç»˜å®¢æˆ·æ¸´æœ›çš„æœªæ¥ã€‚*`;
    } else {
      // Generic mock response
      response = `æ„Ÿè°¢æ‚¨çš„æé—®ã€‚

è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„AIå“åº”ï¼Œç”¨äºæµ‹è¯•ç³»ç»ŸåŠŸèƒ½ã€‚å®é™…ä½¿ç”¨æ—¶ï¼Œç³»ç»Ÿå°†è¿æ¥åˆ°çœŸå®çš„AIæœåŠ¡ï¼ˆæ™ºè°±AIï¼‰æ¥æä¾›ä¸“ä¸šçš„å†…å®¹ç”ŸæˆæœåŠ¡ã€‚

å½“å‰æµ‹è¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå› ä¸ºZHIPU_API_KEYå°šæœªé…ç½®ã€‚

è¦ä½¿ç”¨çœŸå®AIæœåŠ¡ï¼Œè¯·åœ¨backend/.envæ–‡ä»¶ä¸­è®¾ç½®æœ‰æ•ˆçš„ZHIPU_API_KEYã€‚

åŠŸèƒ½æµ‹è¯•å®Œæˆï¼`;
    }

    return response;
  }
}
