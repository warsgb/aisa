import { Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import OpenAI from 'openai';

export interface Message {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

export interface StreamOptions {
  messages: Message[];
  system?: string;
  maxTokens?: number;
  temperature?: number;
  model?: string;
  onChunk?: (chunk: string) => void;
  onComplete?: (fullText: string) => void;
  onError?: (error: Error) => void;
  onStart?: () => void;
}

@Injectable()
export class AIService {
  private readonly logger = new Logger(AIService.name);
  private client: OpenAI;
  private readonly provider: string;
  private readonly defaultModel: string;
  private readonly defaultMaxTokens: number;
  private readonly defaultTemperature: number;

  constructor(private configService: ConfigService) {
    this.provider = this.configService.get<string>('AI_PROVIDER', 'zhipu');

    if (this.provider === 'zhipu') {
      const apiKey = this.configService.get<string>('ZHIPU_API_KEY');
      const baseURL = this.configService.get<string>('ZHIPU_BASE_URL', 'https://open.bigmodel.cn/api/paas/v4/');
      this.defaultModel = this.configService.get<string>('ZHIPU_MODEL', 'glm-4-plus');
      this.defaultMaxTokens = this.configService.get<number>('ZHIPU_MAX_TOKENS', 4096);
      this.defaultTemperature = this.configService.get<number>('ZHIPU_TEMPERATURE', 0.7);

      if (!apiKey || apiKey === 'your_zhipu_api_key_here') {
        this.logger.warn('ZHIPU_API_KEY not configured or using placeholder. Will use mock response mode.');
        this.client = null as any; // Mark as not configured
      } else {
        // æ™ºè°±AIä½¿ç”¨OpenAIå…¼å®¹æ¥å£
        this.client = new OpenAI({
          apiKey,
          baseURL,
        });
      }

      this.logger.log(`AI Service initialized (Zhipu GLM) with model: ${this.defaultModel}`);
      this.logger.log(`Base URL: ${baseURL}`);
    } else {
      // Fallback to Anthropic (if needed in future)
      const apiKey = this.configService.get<string>('ANTHROPIC_API_KEY');
      const baseURL = this.configService.get<string>('ANTHROPIC_BASE_URL');
      this.defaultModel = this.configService.get<string>('ANTHROPIC_MODEL', 'claude-3-5-sonnet-20241022');

      if (!apiKey) {
        this.logger.warn('ANTHROPIC_API_KEY not configured');
      }

      const clientConfig: any = { apiKey };
      if (baseURL) {
        clientConfig.baseURL = baseURL;
      }

      // Note: This would need Anthropic SDK, for now using OpenAI-compatible format
      this.client = new OpenAI(clientConfig);
      this.logger.log(`AI Service initialized (Anthropic-compatible) with model: ${this.defaultModel}`);
    }
  }

  async stream(options: StreamOptions): Promise<string> {
    console.log('ğŸ¤– [AI Service] Stream called with options:', {
      model: options.model,
      maxTokens: options.maxTokens,
      temperature: options.temperature,
      messagesCount: options.messages?.length || 0,
    });

    const {
      messages,
      system,
      maxTokens = this.defaultMaxTokens,
      temperature = this.defaultTemperature,
      model = this.defaultModel,
      onChunk,
      onComplete,
      onError,
      onStart,
    } = options;

    try {
      // Check if AI client is configured
      if (!this.client) {
        this.logger.warn('AI client not configured, using mock response');

        // Generate a mock response
        const mockResponse = this.generateMockResponse(messages, system);

        // Simulate streaming by sending chunks
        if (onStart) {
          onStart();
        }

        if (onChunk) {
          const chunkSize = 20;
          for (let i = 0; i < mockResponse.length; i += chunkSize) {
            const chunk = mockResponse.substring(i, i + chunkSize);
            onChunk(chunk);
            // Add small delay to simulate streaming
            await new Promise(resolve => setTimeout(resolve, 50));
          }
        }

        if (onComplete) {
          onComplete(mockResponse);
        }

        return mockResponse;
      }

      // Prepare messages
      const allMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [];

      if (this.provider === 'zhipu') {
        // æ™ºè°±AI: å°†system promptåˆå¹¶åˆ°ç¬¬ä¸€æ¡useræ¶ˆæ¯ä¸­
        // å› ä¸ºæ™ºè°±AIçš„APIä¸æ”¯æŒæˆ–ä¸éœ€è¦systemè§’è‰²
        const filteredMessages = messages
          .filter((m) => m.role !== 'system')
          .map((m) => ({
            role: m.role as 'user' | 'assistant',
            content: m.content,
          }));

        // å¦‚æœæœ‰system promptï¼Œæ·»åŠ åˆ°ç¬¬ä¸€æ¡useræ¶ˆæ¯å‰
        if (system && filteredMessages.length > 0) {
          const firstMessage = filteredMessages[0];
          if (firstMessage.role === 'user') {
            // åˆå¹¶systemå’Œç¬¬ä¸€æ¡useræ¶ˆæ¯
            filteredMessages[0] = {
              role: 'user',
              content: `${system}\n\n${firstMessage.content}`,
            };
          }
        } else if (system && filteredMessages.length === 0) {
          // å¦‚æœåªæœ‰systemæ²¡æœ‰å…¶ä»–æ¶ˆæ¯ï¼Œå°†systemä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯
          filteredMessages.push({
            role: 'user',
            content: system,
          });
        }

        allMessages.push(...filteredMessages);
      } else {
        // å…¶ä»–æä¾›å•†(å¦‚Anthropic): ä½¿ç”¨æ ‡å‡†systemè§’è‰²
        if (system) {
          allMessages.push({
            role: 'system',
            content: system,
          });
        }

        const filteredMessages = messages
          .filter((m) => m.role !== 'system')
          .map((m) => ({
            role: m.role as 'user' | 'assistant',
            content: m.content,
          }));

        allMessages.push(...filteredMessages);
      }

      this.logger.log(`ğŸ“¤ [AI Service] Sending ${allMessages.length} messages to ${this.provider} API`);
      this.logger.debug(`ğŸ“¨ Message preview:`, JSON.stringify(allMessages).substring(0, 200));
      this.logger.debug(`Messages: ${JSON.stringify(allMessages).substring(0, 200)}...`);

      if (onStart) {
        onStart();
      }

      // Call streaming API
      console.log('ğŸŒ [AI Service] Calling Zhipu API:', {
        endpoint: this.client.baseURL,
        model,
        maxTokens,
      });

      const stream = await this.client.chat.completions.create({
        model,
        messages: allMessages,
        max_tokens: maxTokens,
        temperature,
        stream: true,
      });

      let fullText = '';
      let chunkCount = 0;

      console.log('ğŸ¬ [AI Service] Starting to receive stream...');

      for await (const chunk of stream) {
        const delta = chunk.choices[0]?.delta?.content || '';
        fullText += delta;
        chunkCount++;

        if (chunkCount % 10 === 0) {
          console.log(`ğŸ“Š [AI Service] Received ${chunkCount} chunks`);
        }

        if (onChunk) {
          onChunk(delta);
        }
      }

      if (onComplete) {
        console.log('âœ… [AI Service] Stream completed:', {
          totalChunks: chunkCount,
          totalLength: fullText.length,
        });
        onComplete(fullText);
      }

      console.log('ğŸ [AI Service] Returning response to skill executor');

      return fullText;
    } catch (error) {
      this.logger.error('AI API error:', error);
      if (onError) {
        onError(error as Error);
      }
      throw error;
    }
  }

  async create(options: StreamOptions): Promise<string> {
    const {
      messages,
      system,
      maxTokens = this.defaultMaxTokens,
      temperature = this.defaultTemperature,
      model = this.defaultModel,
    } = options;

    try {
      const allMessages: OpenAI.Chat.ChatCompletionMessageParam[] = [];

      if (this.provider === 'zhipu') {
        // æ™ºè°±AI: å°†system promptåˆå¹¶åˆ°ç¬¬ä¸€æ¡useræ¶ˆæ¯ä¸­
        const filteredMessages = messages
          .filter((m) => m.role !== 'system')
          .map((m) => ({
            role: m.role as 'user' | 'assistant',
            content: m.content,
          }));

        // å¦‚æœæœ‰system promptï¼Œæ·»åŠ åˆ°ç¬¬ä¸€æ¡useræ¶ˆæ¯å‰
        if (system && filteredMessages.length > 0) {
          const firstMessage = filteredMessages[0];
          if (firstMessage.role === 'user') {
            filteredMessages[0] = {
              role: 'user',
              content: `${system}\n\n${firstMessage.content}`,
            };
          }
        } else if (system && filteredMessages.length === 0) {
          // å¦‚æœåªæœ‰systemæ²¡æœ‰å…¶ä»–æ¶ˆæ¯ï¼Œå°†systemä½œä¸ºç¬¬ä¸€æ¡æ¶ˆæ¯
          filteredMessages.push({
            role: 'user',
            content: system,
          });
        }

        allMessages.push(...filteredMessages);
      } else {
        // å…¶ä»–æä¾›å•†: ä½¿ç”¨æ ‡å‡†systemè§’è‰²
        if (system) {
          allMessages.push({
            role: 'system',
            content: system,
          });
        }

        allMessages.push(
          ...messages
            .filter((m) => m.role !== 'system')
            .map((m) => ({
              role: m.role as 'user' | 'assistant',
              content: m.content,
            }))
        );
      }

      const response = await this.client.chat.completions.create({
        model,
        messages: allMessages,
        max_tokens: maxTokens,
        temperature,
      });

      return response.choices[0]?.message?.content || '';
    } catch (error) {
      this.logger.error('AI API error:', error);
      throw error;
    }
  }

  /**
   * æ‰§è¡ŒWebæœç´¢ï¼ˆæ™ºè°±AI WebSearch APIï¼‰
   * @param query æœç´¢æŸ¥è¯¢
   * @param options æœç´¢é€‰é¡¹
   * @returns æœç´¢ç»“æœ
   */
  async webSearch(
    query: string,
    options?: {
      searchEngine?: 'search_std' | 'search_pro' | 'search_pro_sogou' | 'search_pro_quark';
      count?: number;
      searchRecencyFilter?: 'noLimit' | 'day' | 'week' | 'month' | 'year';
      contentSize?: 'low' | 'medium' | 'high';
    },
  ): Promise<{ title: string; link: string; content: string }[]> {
    const {
      searchEngine = 'search_std',
      count = 10,
      searchRecencyFilter = 'noLimit',
      contentSize = 'medium',
    } = options || {};

    this.logger.log(`ğŸ” [WebSearch] Starting search: "${query}"`);
    this.logger.log(`   [WebSearch] Engine: ${searchEngine}, Count: ${count}, Content: ${contentSize}`);

    try {
      // Check if AI client is configured
      if (!this.client) {
        this.logger.warn('[WebSearch] AI client not configured, returning empty search results');
        return [];
      }

      // ä½¿ç”¨æ™ºè°±AIçš„WebSearch API
      // é€šè¿‡ OpenAI å®¢æˆ·ç«¯çš„ chat.completions.create é…åˆ tools å‚æ•°è°ƒç”¨ web_search
      this.logger.log(`ğŸŒ [WebSearch] Calling Zhipu WebSearch API...`);
      const response = await this.client.chat.completions.create({
        model: this.defaultModel,
        messages: [{ role: 'user', content: query }],
        tools: [
          {
            type: 'web_search',
            function: {
              name: 'web_search',
              parameters: {
                search_engine: searchEngine,
                search_query: query,
                count: count,
                search_recency_filter: searchRecencyFilter,
                content_size: contentSize,
              },
            },
          } as any,
        ],
        tool_choice: 'auto',
      });

      // è§£ææœç´¢ç»“æœ
      const toolCalls = response.choices[0]?.message?.tool_calls;
      if (!toolCalls || toolCalls.length === 0) {
        this.logger.warn('[WebSearch] No tool calls returned from API');
        return [];
      }

      this.logger.log(`ğŸ“¥ [WebSearch] Received ${toolCalls.length} tool calls`);

      // æå–æœç´¢ç»“æœ
      const results: { title: string; link: string; content: string }[] = [];

      for (const toolCall of toolCalls) {
        const func = (toolCall as any).function;
        if (func && func.name === 'web_search') {
          try {
            const searchResult = JSON.parse(func.arguments);
            this.logger.log(`ğŸ“Š [WebSearch] Parsed search result, found ${searchResult.results?.length || 0} items`);
            if (searchResult.results && Array.isArray(searchResult.results)) {
              results.push(...searchResult.results);
            }
          } catch (e) {
            this.logger.error('[WebSearch] Failed to parse search result:', e);
          }
        }
      }

      this.logger.log(`âœ… [WebSearch] Completed: "${query}" -> ${results.length} results`);
      return results;
    } catch (error) {
      this.logger.error(`âŒ [WebSearch] API error for "${query}":`, error);
      return []; // è¿”å›ç©ºæ•°ç»„è€Œä¸æ˜¯æŠ›å‡ºé”™è¯¯ï¼Œç¡®ä¿æŠ€èƒ½å¯ä»¥ç»§ç»­æ‰§è¡Œ
    }
  }

  /**
   * æ‰§è¡Œå¤šä¸ªæœç´¢æŸ¥è¯¢å¹¶åˆå¹¶ç»“æœ
   * @param queries æœç´¢æŸ¥è¯¢åˆ—è¡¨
   * @param options æœç´¢é€‰é¡¹
   * @returns åˆå¹¶åçš„æœç´¢ç»“æœ
   */
  async webSearchMultiple(
    queries: string[],
    options?: {
      searchEngine?: 'search_std' | 'search_pro' | 'search_pro_sogou' | 'search_pro_quark';
      count?: number;
      searchRecencyFilter?: 'noLimit' | 'day' | 'week' | 'month' | 'year';
      contentSize?: 'low' | 'medium' | 'high';
    },
  ): Promise<{ query: string; results: { title: string; link: string; content: string }[] }[]> {
    this.logger.log(`ğŸ” [WebSearch] Executing ${queries.length} search queries`);

    const allResults: { query: string; results: { title: string; link: string; content: string }[] }[] = [];

    // é¡ºåºæ‰§è¡Œæœç´¢ï¼Œé¿å…å¹¶å‘é™åˆ¶
    for (const query of queries) {
      const results = await this.webSearch(query, options);
      allResults.push({ query, results });

      // æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
      await new Promise((resolve) => setTimeout(resolve, 200));
    }

    this.logger.log(`âœ… [WebSearch] Completed all ${queries.length} searches`);
    return allResults;
  }

  private generateMockResponse(messages: Message[], system?: string): string {
    // Extract parameters from the last user message if present
    const lastUserMessage = messages.filter((m) => m.role === 'user').pop();
    let response = '';

    if (lastUserMessage && lastUserMessage.content.includes('ç›®æ ‡è§’è‰²')) {
      // Elevator pitch skill mock response
      response = `# ç”µæ¢¯æ¼”è®²ï¼š30ç§’æ‰“åŠ¨CEO

**CEOæ‚¨å¥½ï¼Œç»™æˆ‘30ç§’æ—¶é—´ï¼š**

ä½œä¸ºå»ºç­‘è¡Œä¸šçš„é¢†å†›ä¼ä¸šï¼ŒåŒ—äº¬å»ºå·¥é›†å›¢æ­£åœ¨æ¨è¿›æ•°å­—åŒ–è½¬å‹ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæ‚¨çš„å›¢é˜Ÿèƒ½å¤Ÿï¼š

âœ¨ **æå‡3å€å·¥ä½œæ•ˆç‡** - æ™ºèƒ½æ–‡æ¡£åä½œï¼Œè®©é¡¹ç›®èµ„æ–™å®æ—¶åŒæ­¥
ğŸš€ **ç¼©çŸ­50%å®¡æ‰¹å‘¨æœŸ** - æµç¨‹è‡ªåŠ¨åŒ–ï¼Œä»ç«‹é¡¹åˆ°éªŒæ”¶å…¨é¢æé€Ÿ
ğŸ’¡ **é™ä½70%æ²Ÿé€šæˆæœ¬** - è·¨éƒ¨é—¨åä½œæ— ç¼è¡”æ¥ï¼Œä¿¡æ¯é›¶å»¶è¿Ÿ

WPS 365å·²æœåŠ¡è¶…è¿‡500å®¶å»ºç­‘å›½ä¼ï¼ŒåŒ…æ‹¬ä¸­å»ºã€ä¸­é“ç­‰é¾™å¤´ä¼ä¸šã€‚æˆ‘ä»¬çš„å¹³å°æ­£åœ¨å¸®åŠ©æ‚¨çš„åŒè¡Œå®ç°**"é™æœ¬å¢æ•ˆã€å®‰å…¨å¯æ§"**çš„æ•°å­—åŒ–ç›®æ ‡ã€‚

**ä¸‹å‘¨ä¸€ä¸Šåˆ10ç‚¹ï¼Œæˆ‘èƒ½ç”¨15åˆ†é’Ÿä¸ºæ‚¨å±•ç¤ºå…·ä½“æ¡ˆä¾‹å—ï¼Ÿ**

---
*è¿™å°±æ˜¯æ„¿æ™¯å‹é’©å­çš„åŠ›é‡ - ä¸æ˜¯æ¨é”€äº§å“ï¼Œè€Œæ˜¯æç»˜å®¢æˆ·æ¸´æœ›çš„æœªæ¥ã€‚*`;
    } else {
      // Generic mock response
      response = `æ„Ÿè°¢æ‚¨çš„æé—®ã€‚

è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„AIå“åº”ï¼Œç”¨äºæµ‹è¯•ç³»ç»ŸåŠŸèƒ½ã€‚å®é™…ä½¿ç”¨æ—¶ï¼Œç³»ç»Ÿå°†è¿æ¥åˆ°çœŸå®çš„AIæœåŠ¡ï¼ˆæ™ºè°±AIï¼‰æ¥æä¾›ä¸“ä¸šçš„å†…å®¹ç”ŸæˆæœåŠ¡ã€‚

å½“å‰æµ‹è¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå› ä¸ºZHIPU_API_KEYå°šæœªé…ç½®ã€‚

è¦ä½¿ç”¨çœŸå®AIæœåŠ¡ï¼Œè¯·åœ¨backend/.envæ–‡ä»¶ä¸­è®¾ç½®æœ‰æ•ˆçš„ZHIPU_API_KEYã€‚

åŠŸèƒ½æµ‹è¯•å®Œæˆï¼`;
    }

    return response;
  }
}
